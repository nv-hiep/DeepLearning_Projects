{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "Next_Word_Prediction_multiple_outputs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Next Word Prediction with LSTM"
      ],
      "metadata": {
        "id": "GDtdIJStIYDZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build a model to predict the next word of your text using Long Short Term Memory (LSTM)\n",
        "\n",
        "Dataset for training: https://www.gutenberg.org/files/1342/1342-0.txt"
      ],
      "metadata": {
        "id": "KxPuPnk_J_Lv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "cWuSA7nQf9hu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "    IS_COLAB = True\n",
        "except Exception:\n",
        "    IS_COLAB = False\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
        "    if IS_COLAB:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
        "\n",
        "# Common imports\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import glob\n",
        "import string\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.image as mpimg\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)"
      ],
      "outputs": [],
      "metadata": {
        "id": "XFG_8DVygAfc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from sklearn.preprocessing import LabelBinarizer, LabelEncoder"
      ],
      "outputs": [],
      "metadata": {
        "id": "TZ2YPHM4RPKK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "# Check if NVIDIA GPU is enabled\n",
        "!nvidia-smi"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Oct  2 07:05:45 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.74       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P8    29W / 149W |      3MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqoUhczszmFI",
        "outputId": "773a5f10-0399-42a0-e7bb-21962bde8bfc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data path"
      ],
      "metadata": {
        "id": "mmBND0Bnzke_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "DATA_PATH = '/content'\n",
        "DATA_DIR  = os.path.join(DATA_PATH, 'sample_data')\n",
        "DATA_DIR"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/sample_data'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "metadata": {
        "id": "TYo5OiKIHJHp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8d27cb76-d39a-414d-928e-57f78ac15ed7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "if not os.path.exists(DATA_DIR):\n",
        "  os.mkdir(DATA_DIR)"
      ],
      "outputs": [],
      "metadata": {
        "id": "HZzbexrcsk86"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "os.listdir(DATA_DIR)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQRPkcuqgRDJ",
        "outputId": "6801ad51-a042-4d3b-eb58-2cc93010bbbf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "filepaths = glob.glob(os.path.join(DATA_DIR, '*.txt'))\n",
        "print(len(filepaths))\n",
        "filepaths[:3]"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/sample_data/pride_and_prejudice.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_vFCmb6HFzP",
        "outputId": "b2b3387d-3a28-48ef-918c-73a501400f6d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read data"
      ],
      "metadata": {
        "id": "AjZQeC5re1qR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "content = ''\n",
        "for x in filepaths:\n",
        "  xcontent = open(x, 'r').read().lower()\n",
        "  content = content + xcontent + ' '\n",
        "\n",
        "content[:1000]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ufeffthe project gutenberg ebook of pride and prejudice, by jane austen\\n\\nthis ebook is for the use of anyone anywhere in the united states and\\nmost other parts of the world at no cost and with almost no restrictions\\nwhatsoever. you may copy it, give it away or re-use it under the terms\\nof the project gutenberg license included with this ebook or online at\\nwww.gutenberg.org. if you are not located in the united states, you\\nwill have to check the laws of the country where you are located before\\nusing this ebook.\\n\\ntitle: pride and prejudice\\n\\nauthor: jane austen\\n\\nrelease date: june, 1998 [ebook #1342]\\n[most recently updated: august 23, 2021]\\n\\nlanguage: english\\n\\ncharacter set encoding: utf-8\\n\\nproduced by: anonymous volunteers and david widger\\n\\n*** start of the project gutenberg ebook pride and prejudice ***\\n\\n\\n\\n\\nthere is an illustrated edition of this title which may viewed at ebook\\n[# 42671 ]\\n\\ncover\\n\\n\\n\\n\\npride and prejudice\\n\\nby jane austen\\n\\ncontents\\n\\n  chapter 1\\n\\n  chapter 2\\n\\n  chapter 3\\n\\n  chap'"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "id": "mp7syTqwt7nT",
        "outputId": "34b91a35-2f8a-4c93-b9f8-b50245391049"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove some unnecessary text"
      ],
      "metadata": {
        "id": "XHnZ1exO0JPE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "#replace unnecessary stuff with space\n",
        "content = content.replace('\\n', ' ')\n",
        "content = content.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '').replace('“','').replace('”','')  # new line, carriage return, unicode character --> replace by space\n",
        "content = re.sub(r'[^\\w\\s]', '', content)\n",
        "content = content.replace(\"_\", ' ')\n",
        "\n",
        "#remove unnecessary spaces \n",
        "content = content.split()\n",
        "content = ' '.join(content)\n",
        "content[:1000]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the project gutenberg ebook of pride and prejudice by jane austen this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with almost no restrictions whatsoever you may copy it give it away or reuse it under the terms of the project gutenberg license included with this ebook or online at wwwgutenbergorg if you are not located in the united states you will have to check the laws of the country where you are located before using this ebook title pride and prejudice author jane austen release date june 1998 ebook 1342 most recently updated august 23 2021 language english character set encoding utf8 produced by anonymous volunteers and david widger start of the project gutenberg ebook pride and prejudice there is an illustrated edition of this title which may viewed at ebook 42671 cover pride and prejudice by jane austen contents chapter 1 chapter 2 chapter 3 chapter 4 chapter 5 chapter 6 chapter 7 chapter 8 chapter 9 chapter 10 ch'"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "id": "BejjSYMyhfPl",
        "outputId": "b38be2b9-06cf-4b00-8ef6-6134b15d686d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "len(content)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "677481"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gs7y8CdhiWq9",
        "outputId": "ec0e6788-e4fe-4ed5-b06e-dc42385f4f42"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "content[:50]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the project gutenberg ebook of pride and prejudice'"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bJMHQgFujJ2P",
        "outputId": "c4e3dfbe-8512-44f0-bca4-417708b70105"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "BLhdI-lWWH0G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "# Words to integers\n",
        "# E.g: the project gutenberg ebook of pride and prejudice\n",
        "#  => [1, 2, 4, 5, 5, ....]\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([content])\n",
        "\n",
        "# saving the tokenizer for predict function\n",
        "pickle.dump(tokenizer, open('token.pkl', 'wb'))\n",
        "\n",
        "sequence_data = tokenizer.texts_to_sequences([content])[0]\n",
        "sequence_data[:15]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 176, 482, 911, 3, 321, 4, 1165, 30, 72, 2500, 41, 911, 23, 21]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7me--BDijMr",
        "outputId": "9cb35a26-ec5e-4d12-889d-a323af06daf1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "len(sequence_data)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "124737"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSTd3ecvi3pD",
        "outputId": "a293373b-d204-49a7-9d48-ed6fa82a5ad6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "vocab_size"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7097"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhZurIgok0yJ",
        "outputId": "afeba6ba-2517-4960-ad5c-1f20a28182b9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "sequences = []\n",
        "\n",
        "for i in range(3, len(sequence_data)):\n",
        "    words = sequence_data[i-3:i+1]\n",
        "    sequences.append(words)\n",
        "    \n",
        "print(\"The Length of sequences are: \", len(sequences))\n",
        "sequences = np.array(sequences)\n",
        "sequences[:10]"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Length of sequences are:  124734\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   1,  176,  482,  911],\n",
              "       [ 176,  482,  911,    3],\n",
              "       [ 482,  911,    3,  321],\n",
              "       [ 911,    3,  321,    4],\n",
              "       [   3,  321,    4, 1165],\n",
              "       [ 321,    4, 1165,   30],\n",
              "       [   4, 1165,   30,   72],\n",
              "       [1165,   30,   72, 2500],\n",
              "       [  30,   72, 2500,   41],\n",
              "       [  72, 2500,   41,  911]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sY3GNH6Ik6vb",
        "outputId": "08c6a5aa-6a20-4bc6-a3b5-ea1c14884bd6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature X and label y"
      ],
      "metadata": {
        "id": "84iqz-cfWW9o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in sequences:\n",
        "  X.append(i[0:3])\n",
        "  y.append(i[3])\n",
        "    \n",
        "X = np.array(X)\n",
        "y = np.array(y)"
      ],
      "outputs": [],
      "metadata": {
        "id": "UGUUg0VWl4bp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "source": [
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(124734, 3)\n",
            "(124734,)\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Y0C9ONRl7iQ",
        "outputId": "fe91c688-1c46-49c8-adc1-bacc6902a652"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "source": [
        "print(\"Data: \", X[:10])\n",
        "print(\"Labels: \", y[:10])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data:  [[   1  176  482]\n",
            " [ 176  482  911]\n",
            " [ 482  911    3]\n",
            " [ 911    3  321]\n",
            " [   3  321    4]\n",
            " [ 321    4 1165]\n",
            " [   4 1165   30]\n",
            " [1165   30   72]\n",
            " [  30   72 2500]\n",
            " [  72 2500   41]]\n",
            "Labels:  [ 911    3  321    4 1165   30   72 2500   41  911]\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-_cB-5emEP1",
        "outputId": "6e3e2f27-d4f7-4c5f-aaaf-07cb4cf2b2da"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "source": [
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "y[:5]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwQ-pgtqmO4v",
        "outputId": "ec15f240-f02c-4f24-ecde-694b2fdd97c3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "source": [
        "y.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(124734, 7097)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0y3ngRPmSXQ",
        "outputId": "14227547-b4b5-45f1-cc9e-c2ff6826753f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "source": [
        "# label_encoder = LabelBinarizer()\n",
        "# label_encoder.fit(y)\n",
        "# y_labels = label_encoder.transform(y)\n",
        "# y_labels.shape"
      ],
      "outputs": [],
      "metadata": {
        "id": "gpg0b3w8gxfp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "source": [
        "vocab_size"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7097"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lG2NZZi8FgrW",
        "outputId": "c3bbdea5-2cec-4aee-e34a-2a48984df1f5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "0DQQk5MvR4RT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "source": [
        "# The learned embedding needs to know how many dimensions will be used to represent each word. \n",
        "# That is, the size of the embedding vector space. That is, the size of the embedding vector space.\n",
        "# Common values are 50, 100, and 300. Consider testing smaller or larger values.\n",
        "dimensions_to_represent_word = 256\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length=3))\n",
        "\n",
        "# We will use a two LSTM hidden layers with 100 memory cells each. \n",
        "# More memory cells and a deeper network may achieve better results.\n",
        "model.add(LSTM(dimensions_to_represent_word, return_sequences=True))\n",
        "model.add(LSTM(dimensions_to_represent_word))\n",
        "model.add(Dense(dimensions_to_represent_word, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "model.summary()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 3, 10)             70970     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 3, 256)            273408    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 7097)              1823929   \n",
            "=================================================================\n",
            "Total params: 2,759,411\n",
            "Trainable params: 2,759,411\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8kdcmKZztin",
        "outputId": "9abc3888-3877-4a24-8e4a-3a90d7081747"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"next_words.h5\", monitor='loss', verbose=1, save_best_only=True)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])\n",
        "history = model.fit(X, y, validation_split=0.1, batch_size=64, epochs=200, callbacks=[checkpoint], shuffle=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "1755/1755 [==============================] - 33s 17ms/step - loss: 2.4897 - accuracy: 0.7208 - val_loss: 16.2125 - val_accuracy: 0.0816\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.48974, saving model to next_words.h5\n",
            "Epoch 2/200\n",
            "1755/1755 [==============================] - 27s 16ms/step - loss: 2.1981 - accuracy: 0.7275 - val_loss: 15.1076 - val_accuracy: 0.0885\n",
            "\n",
            "Epoch 00002: loss improved from 2.48974 to 2.19815, saving model to next_words.h5\n",
            "Epoch 3/200\n",
            "1755/1755 [==============================] - 27s 16ms/step - loss: 2.0394 - accuracy: 0.7352 - val_loss: 14.4573 - val_accuracy: 0.0940\n",
            "\n",
            "Epoch 00003: loss improved from 2.19815 to 2.03939, saving model to next_words.h5\n",
            "Epoch 4/200\n",
            "1755/1755 [==============================] - 27s 16ms/step - loss: 1.9209 - accuracy: 0.7401 - val_loss: 13.9094 - val_accuracy: 0.0979\n",
            "\n",
            "Epoch 00004: loss improved from 2.03939 to 1.92090, saving model to next_words.h5\n",
            "Epoch 5/200\n",
            "1755/1755 [==============================] - 27s 15ms/step - loss: 1.8249 - accuracy: 0.7426 - val_loss: 13.5346 - val_accuracy: 0.1024\n",
            "\n",
            "Epoch 00005: loss improved from 1.92090 to 1.82493, saving model to next_words.h5\n",
            "Epoch 6/200\n",
            "1755/1755 [==============================] - 27s 16ms/step - loss: 1.7429 - accuracy: 0.7458 - val_loss: 13.2332 - val_accuracy: 0.1069\n",
            "\n",
            "Epoch 00006: loss improved from 1.82493 to 1.74291, saving model to next_words.h5\n",
            "Epoch 7/200\n",
            "1755/1755 [==============================] - 27s 15ms/step - loss: 1.6712 - accuracy: 0.7479 - val_loss: 12.9896 - val_accuracy: 0.1119\n",
            "\n",
            "Epoch 00007: loss improved from 1.74291 to 1.67116, saving model to next_words.h5\n",
            "Epoch 8/200\n",
            "1755/1755 [==============================] - 27s 16ms/step - loss: 1.6076 - accuracy: 0.7499 - val_loss: 12.7664 - val_accuracy: 0.1170\n",
            "\n",
            "Epoch 00008: loss improved from 1.67116 to 1.60759, saving model to next_words.h5\n",
            "Epoch 9/200\n",
            "1755/1755 [==============================] - 27s 15ms/step - loss: 1.5501 - accuracy: 0.7517 - val_loss: 12.5530 - val_accuracy: 0.1232\n",
            "\n",
            "Epoch 00009: loss improved from 1.60759 to 1.55012, saving model to next_words.h5\n",
            "Epoch 10/200\n",
            "1755/1755 [==============================] - 27s 15ms/step - loss: 1.4975 - accuracy: 0.7534 - val_loss: 12.3369 - val_accuracy: 0.1276\n",
            "\n",
            "Epoch 00010: loss improved from 1.55012 to 1.49752, saving model to next_words.h5\n",
            "Epoch 11/200\n",
            "1755/1755 [==============================] - 27s 15ms/step - loss: 1.4491 - accuracy: 0.7553 - val_loss: 12.2259 - val_accuracy: 0.1342\n",
            "\n",
            "Epoch 00011: loss improved from 1.49752 to 1.44911, saving model to next_words.h5\n",
            "Epoch 12/200\n",
            "1755/1755 [==============================] - 27s 15ms/step - loss: 1.4047 - accuracy: 0.7571 - val_loss: 12.1368 - val_accuracy: 0.1392\n",
            "\n",
            "Epoch 00012: loss improved from 1.44911 to 1.40472, saving model to next_words.h5\n",
            "Epoch 13/200\n",
            "1755/1755 [==============================] - 27s 15ms/step - loss: 1.3637 - accuracy: 0.7587 - val_loss: 11.9925 - val_accuracy: 0.1451\n",
            "\n",
            "Epoch 00013: loss improved from 1.40472 to 1.36370, saving model to next_words.h5\n",
            "Epoch 14/200\n",
            "1755/1755 [==============================] - 27s 15ms/step - loss: 1.3248 - accuracy: 0.7605 - val_loss: 11.9207 - val_accuracy: 0.1505\n",
            "\n",
            "Epoch 00014: loss improved from 1.36370 to 1.32484, saving model to next_words.h5\n",
            "Epoch 15/200\n",
            "1755/1755 [==============================] - 27s 15ms/step - loss: 1.2884 - accuracy: 0.7620 - val_loss: 11.7962 - val_accuracy: 0.1564\n",
            "\n",
            "Epoch 00015: loss improved from 1.32484 to 1.28835, saving model to next_words.h5\n",
            "Epoch 16/200\n",
            "1755/1755 [==============================] - 27s 15ms/step - loss: 1.2545 - accuracy: 0.7635 - val_loss: 11.7579 - val_accuracy: 0.1617\n",
            "\n",
            "Epoch 00016: loss improved from 1.28835 to 1.25447, saving model to next_words.h5\n",
            "Epoch 17/200\n",
            "1755/1755 [==============================] - 27s 15ms/step - loss: 1.2228 - accuracy: 0.7654 - val_loss: 11.7040 - val_accuracy: 0.1693\n",
            "\n",
            "Epoch 00017: loss improved from 1.25447 to 1.22276, saving model to next_words.h5\n",
            "Epoch 18/200\n",
            "1755/1755 [==============================] - 27s 15ms/step - loss: 1.1928 - accuracy: 0.7673 - val_loss: 11.6161 - val_accuracy: 0.1749\n",
            "\n",
            "Epoch 00018: loss improved from 1.22276 to 1.19280, saving model to next_words.h5\n",
            "Epoch 19/200\n",
            "1755/1755 [==============================] - 27s 15ms/step - loss: 1.1635 - accuracy: 0.7696 - val_loss: 11.5336 - val_accuracy: 0.1804\n",
            "\n",
            "Epoch 00019: loss improved from 1.19280 to 1.16348, saving model to next_words.h5\n",
            "Epoch 20/200\n",
            "1755/1755 [==============================] - 27s 15ms/step - loss: 1.1356 - accuracy: 0.7705 - val_loss: 11.5146 - val_accuracy: 0.1877\n",
            "\n",
            "Epoch 00020: loss improved from 1.16348 to 1.13564, saving model to next_words.h5\n",
            "Epoch 21/200\n",
            "1755/1755 [==============================] - 27s 15ms/step - loss: 1.1095 - accuracy: 0.7719 - val_loss: 11.4757 - val_accuracy: 0.1940\n",
            "\n",
            "Epoch 00021: loss improved from 1.13564 to 1.10951, saving model to next_words.h5\n",
            "Epoch 22/200\n",
            "1755/1755 [==============================] - 27s 15ms/step - loss: 1.0843 - accuracy: 0.7742 - val_loss: 11.4134 - val_accuracy: 0.1990\n",
            "\n",
            "Epoch 00022: loss improved from 1.10951 to 1.08427, saving model to next_words.h5\n",
            "Epoch 23/200\n",
            "1755/1755 [==============================] - 27s 16ms/step - loss: 1.0604 - accuracy: 0.7757 - val_loss: 11.3797 - val_accuracy: 0.2060\n",
            "\n",
            "Epoch 00023: loss improved from 1.08427 to 1.06037, saving model to next_words.h5\n",
            "Epoch 24/200\n",
            "1755/1755 [==============================] - 27s 16ms/step - loss: 1.0371 - accuracy: 0.7777 - val_loss: 11.3773 - val_accuracy: 0.2116\n",
            "\n",
            "Epoch 00024: loss improved from 1.06037 to 1.03707, saving model to next_words.h5\n",
            "Epoch 25/200\n",
            "1755/1755 [==============================] - 27s 16ms/step - loss: 1.0149 - accuracy: 0.7794 - val_loss: 11.3350 - val_accuracy: 0.2174\n",
            "\n",
            "Epoch 00025: loss improved from 1.03707 to 1.01486, saving model to next_words.h5\n",
            "Epoch 26/200\n",
            "1755/1755 [==============================] - 27s 16ms/step - loss: 0.9937 - accuracy: 0.7815 - val_loss: 11.3313 - val_accuracy: 0.2235\n",
            "\n",
            "Epoch 00026: loss improved from 1.01486 to 0.99372, saving model to next_words.h5\n",
            "Epoch 27/200\n",
            "1755/1755 [==============================] - 27s 15ms/step - loss: 0.9731 - accuracy: 0.7830 - val_loss: 11.3457 - val_accuracy: 0.2296\n",
            "\n",
            "Epoch 00027: loss improved from 0.99372 to 0.97313, saving model to next_words.h5\n",
            "Epoch 28/200\n",
            "1755/1755 [==============================] - 27s 16ms/step - loss: 0.9534 - accuracy: 0.7844 - val_loss: 11.3114 - val_accuracy: 0.2370\n",
            "\n",
            "Epoch 00028: loss improved from 0.97313 to 0.95339, saving model to next_words.h5\n",
            "Epoch 29/200\n",
            "1755/1755 [==============================] - 27s 16ms/step - loss: 0.9348 - accuracy: 0.7860 - val_loss: 11.2931 - val_accuracy: 0.2414\n",
            "\n",
            "Epoch 00029: loss improved from 0.95339 to 0.93482, saving model to next_words.h5\n",
            "Epoch 30/200\n",
            "1755/1755 [==============================] - 27s 16ms/step - loss: 0.9160 - accuracy: 0.7876 - val_loss: 11.3051 - val_accuracy: 0.2477\n",
            "\n",
            "Epoch 00030: loss improved from 0.93482 to 0.91602, saving model to next_words.h5\n",
            "Epoch 31/200\n",
            "1755/1755 [==============================] - 27s 15ms/step - loss: 0.8985 - accuracy: 0.7892 - val_loss: 11.2903 - val_accuracy: 0.2516\n",
            "\n",
            "Epoch 00031: loss improved from 0.91602 to 0.89850, saving model to next_words.h5\n",
            "Epoch 32/200\n",
            "1755/1755 [==============================] - 27s 16ms/step - loss: 0.8818 - accuracy: 0.7903 - val_loss: 11.2892 - val_accuracy: 0.2574\n",
            "\n",
            "Epoch 00032: loss improved from 0.89850 to 0.88177, saving model to next_words.h5\n",
            "Epoch 33/200\n",
            "1755/1755 [==============================] - 27s 16ms/step - loss: 0.8651 - accuracy: 0.7925 - val_loss: 11.2837 - val_accuracy: 0.2630\n",
            "\n",
            "Epoch 00033: loss improved from 0.88177 to 0.86513, saving model to next_words.h5\n",
            "Epoch 34/200\n",
            "1755/1755 [==============================] - 27s 16ms/step - loss: 0.8489 - accuracy: 0.7935 - val_loss: 11.2973 - val_accuracy: 0.2686\n",
            "\n",
            "Epoch 00034: loss improved from 0.86513 to 0.84892, saving model to next_words.h5\n",
            "Epoch 35/200\n",
            "1755/1755 [==============================] - 27s 16ms/step - loss: 0.8338 - accuracy: 0.7943 - val_loss: 11.3001 - val_accuracy: 0.2736\n",
            "\n",
            "Epoch 00035: loss improved from 0.84892 to 0.83381, saving model to next_words.h5\n",
            "Epoch 36/200\n",
            "1755/1755 [==============================] - 27s 16ms/step - loss: 0.8188 - accuracy: 0.7963 - val_loss: 11.3163 - val_accuracy: 0.2781\n",
            "\n",
            "Epoch 00036: loss improved from 0.83381 to 0.81875, saving model to next_words.h5\n",
            "Epoch 37/200\n",
            "1755/1755 [==============================] - 27s 16ms/step - loss: 0.8045 - accuracy: 0.7974 - val_loss: 11.2928 - val_accuracy: 0.2861\n",
            "\n",
            "Epoch 00037: loss improved from 0.81875 to 0.80455, saving model to next_words.h5\n",
            "Epoch 38/200\n",
            "1755/1755 [==============================] - 27s 16ms/step - loss: 0.7904 - accuracy: 0.7998 - val_loss: 11.3144 - val_accuracy: 0.2888\n",
            "\n",
            "Epoch 00038: loss improved from 0.80455 to 0.79037, saving model to next_words.h5\n",
            "Epoch 39/200\n",
            "1755/1755 [==============================] - 27s 15ms/step - loss: 0.7769 - accuracy: 0.8003 - val_loss: 11.3253 - val_accuracy: 0.2952\n",
            "\n",
            "Epoch 00039: loss improved from 0.79037 to 0.77695, saving model to next_words.h5\n",
            "Epoch 40/200\n",
            "1755/1755 [==============================] - 27s 16ms/step - loss: 0.7640 - accuracy: 0.8017 - val_loss: 11.3369 - val_accuracy: 0.2999\n",
            "\n",
            "Epoch 00040: loss improved from 0.77695 to 0.76395, saving model to next_words.h5\n",
            "Epoch 41/200\n",
            "1755/1755 [==============================] - 27s 15ms/step - loss: 0.7510 - accuracy: 0.8036 - val_loss: 11.3307 - val_accuracy: 0.3042\n",
            "\n",
            "Epoch 00041: loss improved from 0.76395 to 0.75097, saving model to next_words.h5\n",
            "Epoch 42/200\n",
            "1755/1755 [==============================] - 27s 16ms/step - loss: 0.7388 - accuracy: 0.8040 - val_loss: 11.3552 - val_accuracy: 0.3087\n",
            "\n",
            "Epoch 00042: loss improved from 0.75097 to 0.73879, saving model to next_words.h5\n",
            "Epoch 43/200\n",
            "1755/1755 [==============================] - 27s 15ms/step - loss: 0.7270 - accuracy: 0.8055 - val_loss: 11.3494 - val_accuracy: 0.3146\n",
            "\n",
            "Epoch 00043: loss improved from 0.73879 to 0.72697, saving model to next_words.h5\n",
            "Epoch 44/200\n",
            "1755/1755 [==============================] - 27s 16ms/step - loss: 0.7152 - accuracy: 0.8069 - val_loss: 11.4276 - val_accuracy: 0.3167\n",
            "\n",
            "Epoch 00044: loss improved from 0.72697 to 0.71515, saving model to next_words.h5\n",
            "Epoch 45/200\n",
            "1755/1755 [==============================] - 27s 15ms/step - loss: 0.7043 - accuracy: 0.8077 - val_loss: 11.4453 - val_accuracy: 0.3216\n",
            "\n",
            "Epoch 00045: loss improved from 0.71515 to 0.70432, saving model to next_words.h5\n",
            "Epoch 46/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.6932 - accuracy: 0.8091 - val_loss: 11.4402 - val_accuracy: 0.3278\n",
            "\n",
            "Epoch 00046: loss improved from 0.70432 to 0.69323, saving model to next_words.h5\n",
            "Epoch 47/200\n",
            "1755/1755 [==============================] - 27s 16ms/step - loss: 0.6825 - accuracy: 0.8100 - val_loss: 11.4689 - val_accuracy: 0.3311\n",
            "\n",
            "Epoch 00047: loss improved from 0.69323 to 0.68253, saving model to next_words.h5\n",
            "Epoch 48/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.6721 - accuracy: 0.8114 - val_loss: 11.4973 - val_accuracy: 0.3352\n",
            "\n",
            "Epoch 00048: loss improved from 0.68253 to 0.67211, saving model to next_words.h5\n",
            "Epoch 49/200\n",
            "1755/1755 [==============================] - 27s 16ms/step - loss: 0.6625 - accuracy: 0.8124 - val_loss: 11.5094 - val_accuracy: 0.3397\n",
            "\n",
            "Epoch 00049: loss improved from 0.67211 to 0.66252, saving model to next_words.h5\n",
            "Epoch 50/200\n",
            "1755/1755 [==============================] - 27s 16ms/step - loss: 0.6525 - accuracy: 0.8133 - val_loss: 11.5158 - val_accuracy: 0.3443\n",
            "\n",
            "Epoch 00050: loss improved from 0.66252 to 0.65253, saving model to next_words.h5\n",
            "Epoch 51/200\n",
            "1755/1755 [==============================] - 27s 15ms/step - loss: 0.6434 - accuracy: 0.8148 - val_loss: 11.5549 - val_accuracy: 0.3489\n",
            "\n",
            "Epoch 00051: loss improved from 0.65253 to 0.64338, saving model to next_words.h5\n",
            "Epoch 52/200\n",
            "1755/1755 [==============================] - 27s 16ms/step - loss: 0.6338 - accuracy: 0.8163 - val_loss: 11.5805 - val_accuracy: 0.3520\n",
            "\n",
            "Epoch 00052: loss improved from 0.64338 to 0.63382, saving model to next_words.h5\n",
            "Epoch 53/200\n",
            "1755/1755 [==============================] - 27s 16ms/step - loss: 0.6255 - accuracy: 0.8169 - val_loss: 11.5673 - val_accuracy: 0.3583\n",
            "\n",
            "Epoch 00053: loss improved from 0.63382 to 0.62550, saving model to next_words.h5\n",
            "Epoch 54/200\n",
            "1755/1755 [==============================] - 27s 16ms/step - loss: 0.6168 - accuracy: 0.8182 - val_loss: 11.6094 - val_accuracy: 0.3617\n",
            "\n",
            "Epoch 00054: loss improved from 0.62550 to 0.61677, saving model to next_words.h5\n",
            "Epoch 55/200\n",
            "1755/1755 [==============================] - 27s 16ms/step - loss: 0.6082 - accuracy: 0.8191 - val_loss: 11.6456 - val_accuracy: 0.3653\n",
            "\n",
            "Epoch 00055: loss improved from 0.61677 to 0.60824, saving model to next_words.h5\n",
            "Epoch 56/200\n",
            "1755/1755 [==============================] - 27s 16ms/step - loss: 0.6003 - accuracy: 0.8197 - val_loss: 11.6673 - val_accuracy: 0.3674\n",
            "\n",
            "Epoch 00056: loss improved from 0.60824 to 0.60027, saving model to next_words.h5\n",
            "Epoch 57/200\n",
            "1755/1755 [==============================] - 27s 16ms/step - loss: 0.5921 - accuracy: 0.8207 - val_loss: 11.6981 - val_accuracy: 0.3717\n",
            "\n",
            "Epoch 00057: loss improved from 0.60027 to 0.59205, saving model to next_words.h5\n",
            "Epoch 58/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.5849 - accuracy: 0.8222 - val_loss: 11.7079 - val_accuracy: 0.3755\n",
            "\n",
            "Epoch 00058: loss improved from 0.59205 to 0.58493, saving model to next_words.h5\n",
            "Epoch 59/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.5773 - accuracy: 0.8222 - val_loss: 11.7854 - val_accuracy: 0.3788\n",
            "\n",
            "Epoch 00059: loss improved from 0.58493 to 0.57729, saving model to next_words.h5\n",
            "Epoch 60/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.5702 - accuracy: 0.8240 - val_loss: 11.7849 - val_accuracy: 0.3816\n",
            "\n",
            "Epoch 00060: loss improved from 0.57729 to 0.57020, saving model to next_words.h5\n",
            "Epoch 61/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.5633 - accuracy: 0.8245 - val_loss: 11.8403 - val_accuracy: 0.3855\n",
            "\n",
            "Epoch 00061: loss improved from 0.57020 to 0.56330, saving model to next_words.h5\n",
            "Epoch 62/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.5563 - accuracy: 0.8252 - val_loss: 11.8632 - val_accuracy: 0.3884\n",
            "\n",
            "Epoch 00062: loss improved from 0.56330 to 0.55631, saving model to next_words.h5\n",
            "Epoch 63/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.5500 - accuracy: 0.8253 - val_loss: 11.8858 - val_accuracy: 0.3901\n",
            "\n",
            "Epoch 00063: loss improved from 0.55631 to 0.55003, saving model to next_words.h5\n",
            "Epoch 64/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.5435 - accuracy: 0.8268 - val_loss: 11.8972 - val_accuracy: 0.3940\n",
            "\n",
            "Epoch 00064: loss improved from 0.55003 to 0.54351, saving model to next_words.h5\n",
            "Epoch 65/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.5373 - accuracy: 0.8273 - val_loss: 11.9436 - val_accuracy: 0.3956\n",
            "\n",
            "Epoch 00065: loss improved from 0.54351 to 0.53733, saving model to next_words.h5\n",
            "Epoch 66/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.5312 - accuracy: 0.8282 - val_loss: 11.9515 - val_accuracy: 0.3991\n",
            "\n",
            "Epoch 00066: loss improved from 0.53733 to 0.53120, saving model to next_words.h5\n",
            "Epoch 67/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.5256 - accuracy: 0.8282 - val_loss: 12.0187 - val_accuracy: 0.3998\n",
            "\n",
            "Epoch 00067: loss improved from 0.53120 to 0.52562, saving model to next_words.h5\n",
            "Epoch 68/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.5199 - accuracy: 0.8289 - val_loss: 12.0692 - val_accuracy: 0.4035\n",
            "\n",
            "Epoch 00068: loss improved from 0.52562 to 0.51991, saving model to next_words.h5\n",
            "Epoch 69/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.5143 - accuracy: 0.8305 - val_loss: 12.0776 - val_accuracy: 0.4054\n",
            "\n",
            "Epoch 00069: loss improved from 0.51991 to 0.51429, saving model to next_words.h5\n",
            "Epoch 70/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.5091 - accuracy: 0.8300 - val_loss: 12.1052 - val_accuracy: 0.4085\n",
            "\n",
            "Epoch 00070: loss improved from 0.51429 to 0.50907, saving model to next_words.h5\n",
            "Epoch 71/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.5041 - accuracy: 0.8304 - val_loss: 12.1504 - val_accuracy: 0.4088\n",
            "\n",
            "Epoch 00071: loss improved from 0.50907 to 0.50414, saving model to next_words.h5\n",
            "Epoch 72/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.4987 - accuracy: 0.8316 - val_loss: 12.1446 - val_accuracy: 0.4139\n",
            "\n",
            "Epoch 00072: loss improved from 0.50414 to 0.49872, saving model to next_words.h5\n",
            "Epoch 73/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.4943 - accuracy: 0.8316 - val_loss: 12.2042 - val_accuracy: 0.4149\n",
            "\n",
            "Epoch 00073: loss improved from 0.49872 to 0.49425, saving model to next_words.h5\n",
            "Epoch 74/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.4893 - accuracy: 0.8324 - val_loss: 12.2130 - val_accuracy: 0.4180\n",
            "\n",
            "Epoch 00074: loss improved from 0.49425 to 0.48930, saving model to next_words.h5\n",
            "Epoch 75/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.4850 - accuracy: 0.8328 - val_loss: 12.2830 - val_accuracy: 0.4190\n",
            "\n",
            "Epoch 00075: loss improved from 0.48930 to 0.48502, saving model to next_words.h5\n",
            "Epoch 76/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.4808 - accuracy: 0.8338 - val_loss: 12.2965 - val_accuracy: 0.4215\n",
            "\n",
            "Epoch 00076: loss improved from 0.48502 to 0.48076, saving model to next_words.h5\n",
            "Epoch 77/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.4761 - accuracy: 0.8338 - val_loss: 12.3311 - val_accuracy: 0.4230\n",
            "\n",
            "Epoch 00077: loss improved from 0.48076 to 0.47607, saving model to next_words.h5\n",
            "Epoch 78/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.4723 - accuracy: 0.8339 - val_loss: 12.3359 - val_accuracy: 0.4250\n",
            "\n",
            "Epoch 00078: loss improved from 0.47607 to 0.47228, saving model to next_words.h5\n",
            "Epoch 79/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.4683 - accuracy: 0.8349 - val_loss: 12.3765 - val_accuracy: 0.4257\n",
            "\n",
            "Epoch 00079: loss improved from 0.47228 to 0.46829, saving model to next_words.h5\n",
            "Epoch 80/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.4641 - accuracy: 0.8352 - val_loss: 12.3986 - val_accuracy: 0.4278\n",
            "\n",
            "Epoch 00080: loss improved from 0.46829 to 0.46412, saving model to next_words.h5\n",
            "Epoch 81/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.4603 - accuracy: 0.8352 - val_loss: 12.4361 - val_accuracy: 0.4289\n",
            "\n",
            "Epoch 00081: loss improved from 0.46412 to 0.46030, saving model to next_words.h5\n",
            "Epoch 82/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.4568 - accuracy: 0.8360 - val_loss: 12.4925 - val_accuracy: 0.4302\n",
            "\n",
            "Epoch 00082: loss improved from 0.46030 to 0.45681, saving model to next_words.h5\n",
            "Epoch 83/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.4534 - accuracy: 0.8363 - val_loss: 12.4970 - val_accuracy: 0.4316\n",
            "\n",
            "Epoch 00083: loss improved from 0.45681 to 0.45342, saving model to next_words.h5\n",
            "Epoch 84/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.4498 - accuracy: 0.8363 - val_loss: 12.5472 - val_accuracy: 0.4339\n",
            "\n",
            "Epoch 00084: loss improved from 0.45342 to 0.44981, saving model to next_words.h5\n",
            "Epoch 85/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.4465 - accuracy: 0.8367 - val_loss: 12.5844 - val_accuracy: 0.4342\n",
            "\n",
            "Epoch 00085: loss improved from 0.44981 to 0.44654, saving model to next_words.h5\n",
            "Epoch 86/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.4431 - accuracy: 0.8373 - val_loss: 12.5762 - val_accuracy: 0.4370\n",
            "\n",
            "Epoch 00086: loss improved from 0.44654 to 0.44313, saving model to next_words.h5\n",
            "Epoch 87/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.4403 - accuracy: 0.8370 - val_loss: 12.6275 - val_accuracy: 0.4384\n",
            "\n",
            "Epoch 00087: loss improved from 0.44313 to 0.44028, saving model to next_words.h5\n",
            "Epoch 88/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.4373 - accuracy: 0.8376 - val_loss: 12.6685 - val_accuracy: 0.4374\n",
            "\n",
            "Epoch 00088: loss improved from 0.44028 to 0.43729, saving model to next_words.h5\n",
            "Epoch 89/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.4343 - accuracy: 0.8374 - val_loss: 12.6998 - val_accuracy: 0.4399\n",
            "\n",
            "Epoch 00089: loss improved from 0.43729 to 0.43425, saving model to next_words.h5\n",
            "Epoch 90/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.4313 - accuracy: 0.8384 - val_loss: 12.7181 - val_accuracy: 0.4409\n",
            "\n",
            "Epoch 00090: loss improved from 0.43425 to 0.43134, saving model to next_words.h5\n",
            "Epoch 91/200\n",
            "1755/1755 [==============================] - 30s 17ms/step - loss: 0.4284 - accuracy: 0.8388 - val_loss: 12.7627 - val_accuracy: 0.4431\n",
            "\n",
            "Epoch 00091: loss improved from 0.43134 to 0.42837, saving model to next_words.h5\n",
            "Epoch 92/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.4264 - accuracy: 0.8386 - val_loss: 12.7541 - val_accuracy: 0.4441\n",
            "\n",
            "Epoch 00092: loss improved from 0.42837 to 0.42636, saving model to next_words.h5\n",
            "Epoch 93/200\n",
            "1755/1755 [==============================] - 29s 16ms/step - loss: 0.4232 - accuracy: 0.8385 - val_loss: 12.8201 - val_accuracy: 0.4442\n",
            "\n",
            "Epoch 00093: loss improved from 0.42636 to 0.42320, saving model to next_words.h5\n",
            "Epoch 94/200\n",
            "1755/1755 [==============================] - 29s 16ms/step - loss: 0.4210 - accuracy: 0.8389 - val_loss: 12.8219 - val_accuracy: 0.4459\n",
            "\n",
            "Epoch 00094: loss improved from 0.42320 to 0.42100, saving model to next_words.h5\n",
            "Epoch 95/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.4185 - accuracy: 0.8392 - val_loss: 12.8446 - val_accuracy: 0.4466\n",
            "\n",
            "Epoch 00095: loss improved from 0.42100 to 0.41848, saving model to next_words.h5\n",
            "Epoch 96/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.4158 - accuracy: 0.8391 - val_loss: 12.8600 - val_accuracy: 0.4470\n",
            "\n",
            "Epoch 00096: loss improved from 0.41848 to 0.41579, saving model to next_words.h5\n",
            "Epoch 97/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.4132 - accuracy: 0.8398 - val_loss: 12.9225 - val_accuracy: 0.4475\n",
            "\n",
            "Epoch 00097: loss improved from 0.41579 to 0.41321, saving model to next_words.h5\n",
            "Epoch 98/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.4116 - accuracy: 0.8392 - val_loss: 12.9378 - val_accuracy: 0.4489\n",
            "\n",
            "Epoch 00098: loss improved from 0.41321 to 0.41162, saving model to next_words.h5\n",
            "Epoch 99/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.4088 - accuracy: 0.8396 - val_loss: 12.9928 - val_accuracy: 0.4498\n",
            "\n",
            "Epoch 00099: loss improved from 0.41162 to 0.40876, saving model to next_words.h5\n",
            "Epoch 100/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.4068 - accuracy: 0.8396 - val_loss: 12.9936 - val_accuracy: 0.4503\n",
            "\n",
            "Epoch 00100: loss improved from 0.40876 to 0.40677, saving model to next_words.h5\n",
            "Epoch 101/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.4049 - accuracy: 0.8397 - val_loss: 13.0656 - val_accuracy: 0.4497\n",
            "\n",
            "Epoch 00101: loss improved from 0.40677 to 0.40491, saving model to next_words.h5\n",
            "Epoch 102/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.4031 - accuracy: 0.8392 - val_loss: 13.0473 - val_accuracy: 0.4500\n",
            "\n",
            "Epoch 00102: loss improved from 0.40491 to 0.40309, saving model to next_words.h5\n",
            "Epoch 103/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.4006 - accuracy: 0.8397 - val_loss: 13.0695 - val_accuracy: 0.4507\n",
            "\n",
            "Epoch 00103: loss improved from 0.40309 to 0.40058, saving model to next_words.h5\n",
            "Epoch 104/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3989 - accuracy: 0.8399 - val_loss: 13.1237 - val_accuracy: 0.4526\n",
            "\n",
            "Epoch 00104: loss improved from 0.40058 to 0.39887, saving model to next_words.h5\n",
            "Epoch 105/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3972 - accuracy: 0.8405 - val_loss: 13.1581 - val_accuracy: 0.4525\n",
            "\n",
            "Epoch 00105: loss improved from 0.39887 to 0.39719, saving model to next_words.h5\n",
            "Epoch 106/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3956 - accuracy: 0.8399 - val_loss: 13.1905 - val_accuracy: 0.4530\n",
            "\n",
            "Epoch 00106: loss improved from 0.39719 to 0.39557, saving model to next_words.h5\n",
            "Epoch 107/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3936 - accuracy: 0.8404 - val_loss: 13.1771 - val_accuracy: 0.4557\n",
            "\n",
            "Epoch 00107: loss improved from 0.39557 to 0.39362, saving model to next_words.h5\n",
            "Epoch 108/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3918 - accuracy: 0.8404 - val_loss: 13.2218 - val_accuracy: 0.4545\n",
            "\n",
            "Epoch 00108: loss improved from 0.39362 to 0.39184, saving model to next_words.h5\n",
            "Epoch 109/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3904 - accuracy: 0.8405 - val_loss: 13.1994 - val_accuracy: 0.4570\n",
            "\n",
            "Epoch 00109: loss improved from 0.39184 to 0.39039, saving model to next_words.h5\n",
            "Epoch 110/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3884 - accuracy: 0.8407 - val_loss: 13.2722 - val_accuracy: 0.4562\n",
            "\n",
            "Epoch 00110: loss improved from 0.39039 to 0.38838, saving model to next_words.h5\n",
            "Epoch 111/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3869 - accuracy: 0.8409 - val_loss: 13.2995 - val_accuracy: 0.4558\n",
            "\n",
            "Epoch 00111: loss improved from 0.38838 to 0.38693, saving model to next_words.h5\n",
            "Epoch 112/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3858 - accuracy: 0.8406 - val_loss: 13.3053 - val_accuracy: 0.4575\n",
            "\n",
            "Epoch 00112: loss improved from 0.38693 to 0.38576, saving model to next_words.h5\n",
            "Epoch 113/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3842 - accuracy: 0.8407 - val_loss: 13.3780 - val_accuracy: 0.4560\n",
            "\n",
            "Epoch 00113: loss improved from 0.38576 to 0.38419, saving model to next_words.h5\n",
            "Epoch 114/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3825 - accuracy: 0.8412 - val_loss: 13.3626 - val_accuracy: 0.4567\n",
            "\n",
            "Epoch 00114: loss improved from 0.38419 to 0.38250, saving model to next_words.h5\n",
            "Epoch 115/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3812 - accuracy: 0.8410 - val_loss: 13.3895 - val_accuracy: 0.4560\n",
            "\n",
            "Epoch 00115: loss improved from 0.38250 to 0.38119, saving model to next_words.h5\n",
            "Epoch 116/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3801 - accuracy: 0.8415 - val_loss: 13.3973 - val_accuracy: 0.4581\n",
            "\n",
            "Epoch 00116: loss improved from 0.38119 to 0.38013, saving model to next_words.h5\n",
            "Epoch 117/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3786 - accuracy: 0.8412 - val_loss: 13.4609 - val_accuracy: 0.4576\n",
            "\n",
            "Epoch 00117: loss improved from 0.38013 to 0.37858, saving model to next_words.h5\n",
            "Epoch 118/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3769 - accuracy: 0.8412 - val_loss: 13.4467 - val_accuracy: 0.4584\n",
            "\n",
            "Epoch 00118: loss improved from 0.37858 to 0.37685, saving model to next_words.h5\n",
            "Epoch 119/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3763 - accuracy: 0.8417 - val_loss: 13.4642 - val_accuracy: 0.4595\n",
            "\n",
            "Epoch 00119: loss improved from 0.37685 to 0.37626, saving model to next_words.h5\n",
            "Epoch 120/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3745 - accuracy: 0.8408 - val_loss: 13.4841 - val_accuracy: 0.4602\n",
            "\n",
            "Epoch 00120: loss improved from 0.37626 to 0.37450, saving model to next_words.h5\n",
            "Epoch 121/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3737 - accuracy: 0.8417 - val_loss: 13.5394 - val_accuracy: 0.4617\n",
            "\n",
            "Epoch 00121: loss improved from 0.37450 to 0.37366, saving model to next_words.h5\n",
            "Epoch 122/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3724 - accuracy: 0.8413 - val_loss: 13.5419 - val_accuracy: 0.4615\n",
            "\n",
            "Epoch 00122: loss improved from 0.37366 to 0.37236, saving model to next_words.h5\n",
            "Epoch 123/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3715 - accuracy: 0.8408 - val_loss: 13.5501 - val_accuracy: 0.4603\n",
            "\n",
            "Epoch 00123: loss improved from 0.37236 to 0.37148, saving model to next_words.h5\n",
            "Epoch 124/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3699 - accuracy: 0.8413 - val_loss: 13.6122 - val_accuracy: 0.4609\n",
            "\n",
            "Epoch 00124: loss improved from 0.37148 to 0.36994, saving model to next_words.h5\n",
            "Epoch 125/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3691 - accuracy: 0.8404 - val_loss: 13.6199 - val_accuracy: 0.4599\n",
            "\n",
            "Epoch 00125: loss improved from 0.36994 to 0.36911, saving model to next_words.h5\n",
            "Epoch 126/200\n",
            "1755/1755 [==============================] - 29s 16ms/step - loss: 0.3679 - accuracy: 0.8419 - val_loss: 13.6387 - val_accuracy: 0.4617\n",
            "\n",
            "Epoch 00126: loss improved from 0.36911 to 0.36786, saving model to next_words.h5\n",
            "Epoch 127/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3667 - accuracy: 0.8409 - val_loss: 13.6113 - val_accuracy: 0.4614\n",
            "\n",
            "Epoch 00127: loss improved from 0.36786 to 0.36666, saving model to next_words.h5\n",
            "Epoch 128/200\n",
            "1755/1755 [==============================] - 29s 16ms/step - loss: 0.3660 - accuracy: 0.8410 - val_loss: 13.7040 - val_accuracy: 0.4629\n",
            "\n",
            "Epoch 00128: loss improved from 0.36666 to 0.36598, saving model to next_words.h5\n",
            "Epoch 129/200\n",
            "1755/1755 [==============================] - 29s 16ms/step - loss: 0.3649 - accuracy: 0.8415 - val_loss: 13.7173 - val_accuracy: 0.4625\n",
            "\n",
            "Epoch 00129: loss improved from 0.36598 to 0.36487, saving model to next_words.h5\n",
            "Epoch 130/200\n",
            "1755/1755 [==============================] - 29s 16ms/step - loss: 0.3638 - accuracy: 0.8425 - val_loss: 13.7177 - val_accuracy: 0.4629\n",
            "\n",
            "Epoch 00130: loss improved from 0.36487 to 0.36382, saving model to next_words.h5\n",
            "Epoch 131/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3628 - accuracy: 0.8419 - val_loss: 13.7104 - val_accuracy: 0.4630\n",
            "\n",
            "Epoch 00131: loss improved from 0.36382 to 0.36284, saving model to next_words.h5\n",
            "Epoch 132/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3620 - accuracy: 0.8415 - val_loss: 13.7468 - val_accuracy: 0.4625\n",
            "\n",
            "Epoch 00132: loss improved from 0.36284 to 0.36204, saving model to next_words.h5\n",
            "Epoch 133/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3612 - accuracy: 0.8419 - val_loss: 13.8342 - val_accuracy: 0.4638\n",
            "\n",
            "Epoch 00133: loss improved from 0.36204 to 0.36117, saving model to next_words.h5\n",
            "Epoch 134/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3606 - accuracy: 0.8420 - val_loss: 13.8099 - val_accuracy: 0.4635\n",
            "\n",
            "Epoch 00134: loss improved from 0.36117 to 0.36055, saving model to next_words.h5\n",
            "Epoch 135/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3598 - accuracy: 0.8417 - val_loss: 13.8059 - val_accuracy: 0.4630\n",
            "\n",
            "Epoch 00135: loss improved from 0.36055 to 0.35976, saving model to next_words.h5\n",
            "Epoch 136/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3586 - accuracy: 0.8419 - val_loss: 13.8527 - val_accuracy: 0.4634\n",
            "\n",
            "Epoch 00136: loss improved from 0.35976 to 0.35856, saving model to next_words.h5\n",
            "Epoch 137/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3579 - accuracy: 0.8418 - val_loss: 13.8754 - val_accuracy: 0.4641\n",
            "\n",
            "Epoch 00137: loss improved from 0.35856 to 0.35791, saving model to next_words.h5\n",
            "Epoch 138/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3572 - accuracy: 0.8416 - val_loss: 13.8774 - val_accuracy: 0.4646\n",
            "\n",
            "Epoch 00138: loss improved from 0.35791 to 0.35721, saving model to next_words.h5\n",
            "Epoch 139/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3560 - accuracy: 0.8420 - val_loss: 13.9085 - val_accuracy: 0.4652\n",
            "\n",
            "Epoch 00139: loss improved from 0.35721 to 0.35603, saving model to next_words.h5\n",
            "Epoch 140/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3552 - accuracy: 0.8414 - val_loss: 13.9227 - val_accuracy: 0.4649\n",
            "\n",
            "Epoch 00140: loss improved from 0.35603 to 0.35522, saving model to next_words.h5\n",
            "Epoch 141/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3548 - accuracy: 0.8418 - val_loss: 13.9373 - val_accuracy: 0.4649\n",
            "\n",
            "Epoch 00141: loss improved from 0.35522 to 0.35482, saving model to next_words.h5\n",
            "Epoch 142/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3537 - accuracy: 0.8420 - val_loss: 13.9528 - val_accuracy: 0.4646\n",
            "\n",
            "Epoch 00142: loss improved from 0.35482 to 0.35366, saving model to next_words.h5\n",
            "Epoch 143/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3533 - accuracy: 0.8428 - val_loss: 13.9827 - val_accuracy: 0.4655\n",
            "\n",
            "Epoch 00143: loss improved from 0.35366 to 0.35334, saving model to next_words.h5\n",
            "Epoch 144/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3530 - accuracy: 0.8415 - val_loss: 14.0117 - val_accuracy: 0.4649\n",
            "\n",
            "Epoch 00144: loss improved from 0.35334 to 0.35300, saving model to next_words.h5\n",
            "Epoch 145/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3517 - accuracy: 0.8416 - val_loss: 14.0066 - val_accuracy: 0.4662\n",
            "\n",
            "Epoch 00145: loss improved from 0.35300 to 0.35173, saving model to next_words.h5\n",
            "Epoch 146/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3514 - accuracy: 0.8418 - val_loss: 14.0027 - val_accuracy: 0.4667\n",
            "\n",
            "Epoch 00146: loss improved from 0.35173 to 0.35139, saving model to next_words.h5\n",
            "Epoch 147/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3508 - accuracy: 0.8420 - val_loss: 14.0428 - val_accuracy: 0.4651\n",
            "\n",
            "Epoch 00147: loss improved from 0.35139 to 0.35080, saving model to next_words.h5\n",
            "Epoch 148/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3501 - accuracy: 0.8414 - val_loss: 14.0149 - val_accuracy: 0.4657\n",
            "\n",
            "Epoch 00148: loss improved from 0.35080 to 0.35008, saving model to next_words.h5\n",
            "Epoch 149/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3492 - accuracy: 0.8419 - val_loss: 14.1007 - val_accuracy: 0.4668\n",
            "\n",
            "Epoch 00149: loss improved from 0.35008 to 0.34922, saving model to next_words.h5\n",
            "Epoch 150/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3489 - accuracy: 0.8416 - val_loss: 14.1343 - val_accuracy: 0.4661\n",
            "\n",
            "Epoch 00150: loss improved from 0.34922 to 0.34891, saving model to next_words.h5\n",
            "Epoch 151/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3484 - accuracy: 0.8416 - val_loss: 14.1004 - val_accuracy: 0.4659\n",
            "\n",
            "Epoch 00151: loss improved from 0.34891 to 0.34842, saving model to next_words.h5\n",
            "Epoch 152/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3476 - accuracy: 0.8417 - val_loss: 14.1021 - val_accuracy: 0.4680\n",
            "\n",
            "Epoch 00152: loss improved from 0.34842 to 0.34760, saving model to next_words.h5\n",
            "Epoch 153/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3471 - accuracy: 0.8418 - val_loss: 14.1336 - val_accuracy: 0.4665\n",
            "\n",
            "Epoch 00153: loss improved from 0.34760 to 0.34707, saving model to next_words.h5\n",
            "Epoch 154/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3460 - accuracy: 0.8420 - val_loss: 14.1641 - val_accuracy: 0.4670\n",
            "\n",
            "Epoch 00154: loss improved from 0.34707 to 0.34601, saving model to next_words.h5\n",
            "Epoch 155/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3463 - accuracy: 0.8417 - val_loss: 14.1780 - val_accuracy: 0.4676\n",
            "\n",
            "Epoch 00155: loss did not improve from 0.34601\n",
            "Epoch 156/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3453 - accuracy: 0.8420 - val_loss: 14.2236 - val_accuracy: 0.4675\n",
            "\n",
            "Epoch 00156: loss improved from 0.34601 to 0.34533, saving model to next_words.h5\n",
            "Epoch 157/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3451 - accuracy: 0.8412 - val_loss: 14.1486 - val_accuracy: 0.4677\n",
            "\n",
            "Epoch 00157: loss improved from 0.34533 to 0.34510, saving model to next_words.h5\n",
            "Epoch 158/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3439 - accuracy: 0.8422 - val_loss: 14.2134 - val_accuracy: 0.4679\n",
            "\n",
            "Epoch 00158: loss improved from 0.34510 to 0.34391, saving model to next_words.h5\n",
            "Epoch 159/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3439 - accuracy: 0.8420 - val_loss: 14.2167 - val_accuracy: 0.4670\n",
            "\n",
            "Epoch 00159: loss did not improve from 0.34391\n",
            "Epoch 160/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3434 - accuracy: 0.8412 - val_loss: 14.2747 - val_accuracy: 0.4674\n",
            "\n",
            "Epoch 00160: loss improved from 0.34391 to 0.34338, saving model to next_words.h5\n",
            "Epoch 161/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3427 - accuracy: 0.8412 - val_loss: 14.2874 - val_accuracy: 0.4677\n",
            "\n",
            "Epoch 00161: loss improved from 0.34338 to 0.34275, saving model to next_words.h5\n",
            "Epoch 162/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3426 - accuracy: 0.8418 - val_loss: 14.2657 - val_accuracy: 0.4683\n",
            "\n",
            "Epoch 00162: loss improved from 0.34275 to 0.34256, saving model to next_words.h5\n",
            "Epoch 163/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3415 - accuracy: 0.8424 - val_loss: 14.3289 - val_accuracy: 0.4660\n",
            "\n",
            "Epoch 00163: loss improved from 0.34256 to 0.34151, saving model to next_words.h5\n",
            "Epoch 164/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3416 - accuracy: 0.8417 - val_loss: 14.3161 - val_accuracy: 0.4670\n",
            "\n",
            "Epoch 00164: loss did not improve from 0.34151\n",
            "Epoch 165/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3410 - accuracy: 0.8413 - val_loss: 14.2868 - val_accuracy: 0.4677\n",
            "\n",
            "Epoch 00165: loss improved from 0.34151 to 0.34096, saving model to next_words.h5\n",
            "Epoch 166/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3401 - accuracy: 0.8417 - val_loss: 14.3420 - val_accuracy: 0.4678\n",
            "\n",
            "Epoch 00166: loss improved from 0.34096 to 0.34010, saving model to next_words.h5\n",
            "Epoch 167/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3406 - accuracy: 0.8414 - val_loss: 14.3529 - val_accuracy: 0.4680\n",
            "\n",
            "Epoch 00167: loss did not improve from 0.34010\n",
            "Epoch 168/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3398 - accuracy: 0.8420 - val_loss: 14.4112 - val_accuracy: 0.4687\n",
            "\n",
            "Epoch 00168: loss improved from 0.34010 to 0.33979, saving model to next_words.h5\n",
            "Epoch 169/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3392 - accuracy: 0.8421 - val_loss: 14.3190 - val_accuracy: 0.4692\n",
            "\n",
            "Epoch 00169: loss improved from 0.33979 to 0.33919, saving model to next_words.h5\n",
            "Epoch 170/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3390 - accuracy: 0.8415 - val_loss: 14.3756 - val_accuracy: 0.4676\n",
            "\n",
            "Epoch 00170: loss improved from 0.33919 to 0.33899, saving model to next_words.h5\n",
            "Epoch 171/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3385 - accuracy: 0.8420 - val_loss: 14.3890 - val_accuracy: 0.4680\n",
            "\n",
            "Epoch 00171: loss improved from 0.33899 to 0.33851, saving model to next_words.h5\n",
            "Epoch 172/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3384 - accuracy: 0.8417 - val_loss: 14.4361 - val_accuracy: 0.4700\n",
            "\n",
            "Epoch 00172: loss improved from 0.33851 to 0.33840, saving model to next_words.h5\n",
            "Epoch 173/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3379 - accuracy: 0.8415 - val_loss: 14.4275 - val_accuracy: 0.4675\n",
            "\n",
            "Epoch 00173: loss improved from 0.33840 to 0.33793, saving model to next_words.h5\n",
            "Epoch 174/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3374 - accuracy: 0.8415 - val_loss: 14.4453 - val_accuracy: 0.4684\n",
            "\n",
            "Epoch 00174: loss improved from 0.33793 to 0.33741, saving model to next_words.h5\n",
            "Epoch 175/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3369 - accuracy: 0.8422 - val_loss: 14.4348 - val_accuracy: 0.4669\n",
            "\n",
            "Epoch 00175: loss improved from 0.33741 to 0.33689, saving model to next_words.h5\n",
            "Epoch 176/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3367 - accuracy: 0.8415 - val_loss: 14.4433 - val_accuracy: 0.4696\n",
            "\n",
            "Epoch 00176: loss improved from 0.33689 to 0.33673, saving model to next_words.h5\n",
            "Epoch 177/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3362 - accuracy: 0.8417 - val_loss: 14.4968 - val_accuracy: 0.4691\n",
            "\n",
            "Epoch 00177: loss improved from 0.33673 to 0.33616, saving model to next_words.h5\n",
            "Epoch 178/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3361 - accuracy: 0.8417 - val_loss: 14.5188 - val_accuracy: 0.4688\n",
            "\n",
            "Epoch 00178: loss improved from 0.33616 to 0.33609, saving model to next_words.h5\n",
            "Epoch 179/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3353 - accuracy: 0.8418 - val_loss: 14.5323 - val_accuracy: 0.4690\n",
            "\n",
            "Epoch 00179: loss improved from 0.33609 to 0.33533, saving model to next_words.h5\n",
            "Epoch 180/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3352 - accuracy: 0.8419 - val_loss: 14.5156 - val_accuracy: 0.4701\n",
            "\n",
            "Epoch 00180: loss improved from 0.33533 to 0.33524, saving model to next_words.h5\n",
            "Epoch 181/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3348 - accuracy: 0.8419 - val_loss: 14.4964 - val_accuracy: 0.4676\n",
            "\n",
            "Epoch 00181: loss improved from 0.33524 to 0.33477, saving model to next_words.h5\n",
            "Epoch 182/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3345 - accuracy: 0.8418 - val_loss: 14.5498 - val_accuracy: 0.4693\n",
            "\n",
            "Epoch 00182: loss improved from 0.33477 to 0.33452, saving model to next_words.h5\n",
            "Epoch 183/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3344 - accuracy: 0.8414 - val_loss: 14.5502 - val_accuracy: 0.4678\n",
            "\n",
            "Epoch 00183: loss improved from 0.33452 to 0.33439, saving model to next_words.h5\n",
            "Epoch 184/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3338 - accuracy: 0.8415 - val_loss: 14.5442 - val_accuracy: 0.4677\n",
            "\n",
            "Epoch 00184: loss improved from 0.33439 to 0.33377, saving model to next_words.h5\n",
            "Epoch 185/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3334 - accuracy: 0.8422 - val_loss: 14.5600 - val_accuracy: 0.4700\n",
            "\n",
            "Epoch 00185: loss improved from 0.33377 to 0.33340, saving model to next_words.h5\n",
            "Epoch 186/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3332 - accuracy: 0.8417 - val_loss: 14.5691 - val_accuracy: 0.4703\n",
            "\n",
            "Epoch 00186: loss improved from 0.33340 to 0.33317, saving model to next_words.h5\n",
            "Epoch 187/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3327 - accuracy: 0.8415 - val_loss: 14.6307 - val_accuracy: 0.4692\n",
            "\n",
            "Epoch 00187: loss improved from 0.33317 to 0.33274, saving model to next_words.h5\n",
            "Epoch 188/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3325 - accuracy: 0.8420 - val_loss: 14.6260 - val_accuracy: 0.4706\n",
            "\n",
            "Epoch 00188: loss improved from 0.33274 to 0.33249, saving model to next_words.h5\n",
            "Epoch 189/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3324 - accuracy: 0.8423 - val_loss: 14.6094 - val_accuracy: 0.4694\n",
            "\n",
            "Epoch 00189: loss improved from 0.33249 to 0.33236, saving model to next_words.h5\n",
            "Epoch 190/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3322 - accuracy: 0.8421 - val_loss: 14.6435 - val_accuracy: 0.4698\n",
            "\n",
            "Epoch 00190: loss improved from 0.33236 to 0.33222, saving model to next_words.h5\n",
            "Epoch 191/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3317 - accuracy: 0.8415 - val_loss: 14.6415 - val_accuracy: 0.4704\n",
            "\n",
            "Epoch 00191: loss improved from 0.33222 to 0.33172, saving model to next_words.h5\n",
            "Epoch 192/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3316 - accuracy: 0.8416 - val_loss: 14.6382 - val_accuracy: 0.4706\n",
            "\n",
            "Epoch 00192: loss improved from 0.33172 to 0.33155, saving model to next_words.h5\n",
            "Epoch 193/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3309 - accuracy: 0.8418 - val_loss: 14.6638 - val_accuracy: 0.4696\n",
            "\n",
            "Epoch 00193: loss improved from 0.33155 to 0.33085, saving model to next_words.h5\n",
            "Epoch 194/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3310 - accuracy: 0.8422 - val_loss: 14.6470 - val_accuracy: 0.4683\n",
            "\n",
            "Epoch 00194: loss did not improve from 0.33085\n",
            "Epoch 195/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3302 - accuracy: 0.8421 - val_loss: 14.7160 - val_accuracy: 0.4694\n",
            "\n",
            "Epoch 00195: loss improved from 0.33085 to 0.33023, saving model to next_words.h5\n",
            "Epoch 196/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3303 - accuracy: 0.8424 - val_loss: 14.6958 - val_accuracy: 0.4687\n",
            "\n",
            "Epoch 00196: loss did not improve from 0.33023\n",
            "Epoch 197/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3300 - accuracy: 0.8414 - val_loss: 14.7267 - val_accuracy: 0.4702\n",
            "\n",
            "Epoch 00197: loss improved from 0.33023 to 0.33004, saving model to next_words.h5\n",
            "Epoch 198/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3299 - accuracy: 0.8415 - val_loss: 14.6976 - val_accuracy: 0.4722\n",
            "\n",
            "Epoch 00198: loss improved from 0.33004 to 0.32994, saving model to next_words.h5\n",
            "Epoch 199/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3296 - accuracy: 0.8418 - val_loss: 14.7373 - val_accuracy: 0.4715\n",
            "\n",
            "Epoch 00199: loss improved from 0.32994 to 0.32958, saving model to next_words.h5\n",
            "Epoch 200/200\n",
            "1755/1755 [==============================] - 28s 16ms/step - loss: 0.3295 - accuracy: 0.8423 - val_loss: 14.7607 - val_accuracy: 0.4687\n",
            "\n",
            "Epoch 00200: loss improved from 0.32958 to 0.32947, saving model to next_words.h5\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfTfDxb5oLhR",
        "outputId": "3dfa243d-6da8-49b9-e65b-eb6e2cea913b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "source": [
        "print(history.history.keys())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
          ]
        }
      ],
      "metadata": {
        "id": "JnEo73QNx7aN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27d694c1-ebea-4156-cfbd-787553e8d52d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "source": [
        "#  \"Accuracy\"\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "# \"Loss\"\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "metadata": {
        "id": "KU0hdu_H6LmA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 587
        },
        "outputId": "5477e31b-5672-4006-d678-deab036cad0d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not great! Need to improve in future."
      ],
      "metadata": {
        "id": "gDHpbuNkWg0u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save and load the model"
      ],
      "metadata": {
        "id": "A-RQIp8_R8g5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "source": [
        "# tokenizer.word_index.items()\n",
        "# dict_items([('the', 1), ('to', 2), ('of', 3), ('and', 4)...])"
      ],
      "outputs": [],
      "metadata": {
        "id": "przAD0itUuwe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model = load_model('next_words.h5')\n",
        "tokenizer = pickle.load(open('token.pkl', 'rb'))\n",
        "\n",
        "def Predict_Next_Words(model, tokenizer, text):\n",
        "\n",
        "  sequence = tokenizer.texts_to_sequences([text])\n",
        "  sequence = np.array(sequence)\n",
        "  preds = np.argsort(model.predict(sequence))[0][::-1][:5]\n",
        "  # preds = np.argmax(model.predict(sequence))\n",
        "  \n",
        "  predicted_words = []\n",
        "  for key, value in tokenizer.word_index.items():\n",
        "      if value in preds:\n",
        "          predicted_words.append(key)\n",
        "\n",
        "  return predicted_words"
      ],
      "outputs": [],
      "metadata": {
        "id": "USUtwXPopQYd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction"
      ],
      "metadata": {
        "id": "8oB2ZovJR_zI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "source": [
        "text = \"He was quite\"\n",
        "\n",
        "if text == \"0\":\n",
        "  print(\"Execution completed.....\")\n",
        "\n",
        "else:\n",
        "  try:\n",
        "    text = text.split(\" \")\n",
        "    text = text[-3:]\n",
        "    print(text)\n",
        "\n",
        "    pred_words = Predict_Next_Words(model, tokenizer, text)\n",
        "    print('Predicted words: ', pred_words)\n",
        "      \n",
        "  except Exception as e:\n",
        "    print(\"Error occurred: \",e)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['He', 'was', 'quite']\n",
            "Predicted words:  ['a', 'so', 'young', 'happy', 'short']\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4u1HEbCzGTe",
        "outputId": "bc126e60-f0d3-4c15-9292-c9daf27396de"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "source": [
        "texts = [\"She was quite\",\n",
        "         \"thank you for\",\n",
        "         \"She is pretty\",\n",
        "         \"He loves you\",\n",
        "         \"there were six\",\n",
        "         \"I was so\",\n",
        "         \"Any house would\"\n",
        "         ]\n",
        "\n",
        "for text in texts:\n",
        "  if text == \"0\":\n",
        "    print(\"Execution completed.....\")\n",
        "    break\n",
        "\n",
        "  else:\n",
        "    try:\n",
        "      text = text.strip().split(\" \")\n",
        "      text = text[-3:]\n",
        "      print('Input text: ', text)\n",
        "\n",
        "      pred_words = Predict_Next_Words(model, tokenizer, text)\n",
        "      print('Predicted words: ', pred_words)\n",
        "      print()\n",
        "        \n",
        "    except Exception as e:\n",
        "      print(\"Error occurred: \",e)\n",
        "      continue"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input text:  ['She', 'was', 'quite']\n",
            "Predicted words:  ['glad', 'disappointed', 'decided', 'amazed', 'unprepared']\n",
            "\n",
            "Input text:  ['thank', 'you', 'for']\n",
            "Predicted words:  ['you', 'him', 'my', 'me', 'explaining']\n",
            "\n",
            "Input text:  ['She', 'is', 'pretty']\n",
            "Predicted words:  ['by', 'able', 'amiable', 'assured', 'fond']\n",
            "\n",
            "Input text:  ['He', 'loves', 'you']\n",
            "Predicted words:  ['have', 'are', 'must', 'did', 'know']\n",
            "\n",
            "Input text:  ['there', 'were', 'six']\n",
            "Predicted words:  ['of', 'much', 'one', 'other', 'great']\n",
            "\n",
            "Input text:  ['I', 'was', 'so']\n",
            "Predicted words:  ['sure', 'afraid', 'bad', 'vexed', 'frightened']\n",
            "\n",
            "Input text:  ['Any', 'house', 'would']\n",
            "Predicted words:  ['in', 'you', 'be', 'been', 'subsided']\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yjqkSLDpfid",
        "outputId": "21b93d94-e053-4ce5-e10a-3842433810b4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the model"
      ],
      "metadata": {
        "id": "x9grCaVLqztX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "source": [
        "# serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"Next_Word_Pred_model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"Next_Word_Pred_model_weight.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model to disk\n"
          ]
        }
      ],
      "metadata": {
        "id": "1Dkjjhu7spid",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3feb1a60-ccd1-415b-ecc5-8410b9fdba9d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "source": [
        "# save the network to disk\n",
        "print(\"Save the model...\")\n",
        "model.save(\"Next_Word_Pred_model.model\", save_format=\"h5\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Save the model...\n"
          ]
        }
      ],
      "metadata": {
        "id": "e-qtqLwztX_W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58a2c780-f56a-4951-b34a-ec5db416198f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "source": [
        "# # later...\n",
        " \n",
        "# # load json and create model\n",
        "# json_file = open('model.json', 'r')\n",
        "# loaded_model_json = json_file.read()\n",
        "# json_file.close()\n",
        "# loaded_model = model_from_json(loaded_model_json)\n",
        "# # load weights into new model\n",
        "# loaded_model.load_weights(\"model.h5\")\n",
        "# print(\"Loaded model from disk\")\n",
        " \n",
        "# # evaluate loaded model on test data\n",
        "# loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "# score = loaded_model.evaluate(X, Y, verbose=0)\n",
        "# print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
      ],
      "outputs": [],
      "metadata": {
        "id": "9cVcK9RNsxYL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "source": [
        "from google.colab import files\n",
        "files.download(\"Next_Word_Pred_model_classweight.json\")\n",
        "files.download(\"Next_Word_Pred_model_classweight_weight.h5\")\n",
        "files.download(\"Next_Word_Pred_model_classweight.model\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": "download(\"download_5b1a08a8-b798-4c38-92ab-de2037950b1c\", \"Next_Word_Pred_model_classweight.json\", 3213)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": "download(\"download_01a15283-cf9d-4655-a40e-15c5685bc3ea\", \"Next_Word_Pred_model_classweight_weight.h5\", 11062068)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": "download(\"download_43bfe318-020a-4682-b264-82d35d0784bc\", \"Next_Word_Pred_model_classweight.model\", 33169316)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "id": "IbaZHktXj3w4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "0907480f-1ab2-44a5-b371-c0646dd9a23b"
      }
    }
  ]
}